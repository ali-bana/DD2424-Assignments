\documentclass[a4paper]{article}

%%%%%%%% CREATE DOCUMENT STRUCTURE %%%%%%%%
%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{subfig}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{sectsty}
\usepackage{apacite}
\usepackage{float}
\usepackage{titling} 
\usepackage{blindtext}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.0, 0.4, 0.0}
\usepackage{graphicx}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\E}{\mathbb{E}}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}
 



%%%%%%%% DOCUMENT %%%%%%%%
\begin{document}

%%%% Title Page
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 							% horizontal line and its thickness
\center 
 
 
% University

\includegraphics[width=0.15\textwidth]{images/kth_logo.png}\\[0.5cm] 	% University logo

\textsc{\LARGE KTH Royal Institute of Technology}\\[1cm]

% Document info
\textsc{\Large Deep Learning in Datascience}\\[0.2cm]
\textsc{\large DD2424}\\[1cm] 										% Course Code
\HRule \\[0.8cm]
{ \huge \bfseries Assignment 1}\\[0.7cm]								% Assignment
\HRule \\[2cm]
\large
\emph{Authors:}\\
Ali Banaei Mobarak Abadi\\[1.5cm]													% Author info
{\large \today}\\[5cm]

\vfill 
\end{titlepage}

%%\begin{abstract}
%%Your abstract.
%%\end{abstract}

%%%% SECTIONS
%% Section 1
\section{Assignment Report}
In this section, the results we can get by running the code are presented. For an explanation of the code and implementations, please see \autoref{sec:2}. Note that for the rest of this report, the available training data is divided into two parts, validation with a size of 5000 samples and the rest as training.

\subsection{Checking the gradients}

After implementing the code for the forward path and calculating the gradient, calculated gradients were checked using the provided functions (for the final run, the slower version was used). To make the gradients big enough so that we can use the difference without a need for a division, the weight and bias matrices were initialized with a Gaussian distribution with a mean of 0 and a variance of 4. After calculating the gradients using the two methods, the mean, standard deviation, and maximum values for the absolute values of the difference of gradients were calculated by two methods, and the min, max and the SD of numerical gradients were printed. The results were as follows.

\begin{lstlisting}
	For abs of diff of gW: mean: 5.12e-09, std: 3.75e-09, max:2.11e-08, gradient min: -1.72, gradient max: 1.53, gradient std: 0.42
	For abs of diff of gb: mean: 1.60e-09, std: 1.33e-09, max:4.21e-09, gradient min: -0.1, gradient max: 0.064, gradient std: 0.12
\end{lstlisting}

As we can see, the differences are vary small while the gradients are not. So, we can conclude we are calculating the gradients correctly.

\subsection{Training and evaluation}

After implementing the remaining functions, the model was trained and tested using the provided hyperparameters. \autoref{fig:training} depicts the loss, cost function, and accuracy of training and validation sets during training. Also, the results of the evaluation of the model on test data can be found in \autoref{tab:ev}.

\begin{table}[h]
	\centering
	\caption{Evaluation of the model with different values of $\lambda$ on the test set.}
	\label{tab:ev}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		$\lambda$ & $\eta$ & Loss & Cost & Accuracy \\ \hline
		0         & 0.1    & 6.92 & 6.92 & 0.27     \\ \hline
		0         & 0.001  & 2.02 & 2.02 & 0.32     \\ \hline
		0.1       & 0.001  & 1.74 & 1.80 & 0.41     \\ \hline
		1         & 0.001  & 1.85 & 1.92 & 0.37     \\ \hline
	\end{tabular}
\end{table}

\begin{figure}[h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.32\linewidth]{images/loss_over_trainig_eta=0.1_lambda=0.png}
		\includegraphics[width=.32\linewidth]{images/cost_over_trainig_eta=0.1_lambda=0.png}
		\includegraphics[width=.32\linewidth]{images/accuracy_over_trainig_eta=0.1_lambda=0.png}
		\caption{$\lambda=0, \; \eta = 0.1$ }
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.32\linewidth]{images/loss_over_trainig_eta=0.001_lambda=0.png}
		\includegraphics[width=.32\linewidth]{images/cost_over_trainig_eta=0.001_lambda=0.png}
		\includegraphics[width=.32\linewidth]{images/accuracy_over_trainig_eta=0.001_lambda=0.png}
		\caption{$\lambda=0, \; \eta = 0.001$ }
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.32\linewidth]{images/loss_over_trainig_eta=0.001_lambda=0.1.png}
		\includegraphics[width=.32\linewidth]{images/cost_over_trainig_eta=0.001_lambda=0.1.png}
		\includegraphics[width=.32\linewidth]{images/accuracy_over_trainig_eta=0.001_lambda=0.1.png}
		\caption{$\lambda=0.1, \; \eta = 0.001$ }
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.32\linewidth]{images/loss_over_trainig_eta=0.001_lambda=1.png}
		\includegraphics[width=.32\linewidth]{images/cost_over_trainig_eta=0.001_lambda=1.png}
		\includegraphics[width=.32\linewidth]{images/accuracy_over_trainig_eta=0.001_lambda=1.png}
		\caption{$\lambda=1, \; \eta = 0.001$ }
	\end{subfigure}

	\caption{Loss, cost and accuracy of the model during training.}
	\label{fig:training}
\end{figure}

We can see that when we set 0.1 as the learning rate, the network reaches a not-so-bad point after the first iteration; however, it keeps overshooting the optimal values and does not converge to a good set of weights. In this setting, the network jumps around, and it does not have adequate performance in the evaluation. However, when we change the learning rate to 0.001, the network converges, and the final accuracy increases. It is worth mentioning that when we do not use regularization ($\lambda=0$), the validation and training loss and accuracy have a considerable difference, and the network's performance is better on training data that it has seen. As expected, when we increase $\lambda$, this difference decreases, and the generalization performance of the network enhances. However, when we have $\lambda = 1$, the network's performance decreases. It may be because this value would enforce a strong constraint on our network, which structure is very simple. So, the network would not be able to optimize the weights to decrease the loss function effectively.

Also, the visualization of the $W$ can be found in \autoref{fig:w_vis}. As we see, by increasing the $\lambda$, we have a smoother matrix. Also, when we do not use regularization, the network does not seem to capture any repetitive pattern in each class, while by using regularization, instead of memorizing the training set, the model learns these patterns.






\begin{figure}[h]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.5\linewidth]{images/weights_eta=0.1_lambda=0.png}
		\caption{$\lambda=0, \; \eta = 0.1$ }
	\end{subfigure}
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.5\linewidth]{images/weights_eta=0.001_lambda=0.png}
		\caption{$\lambda=0, \; \eta = 0.001$ }
	\end{subfigure}
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.5\linewidth]{images/weights_eta=0.001_lambda=0.1.png}
		\caption{$\lambda=0.1, \; \eta = 0.001$ }
	\end{subfigure}
	
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.5\linewidth]{images/weights_eta=0.001_lambda=1.png}
		\caption{$\lambda=1, \; \eta = 0.001$ }
	\end{subfigure}
	
	\caption{Visualization of W after training.}
	\label{fig:w_vis}
\end{figure}


\section{Implementation Documentation}
\label{sec:2}

Here is a brief explanation of the implemented functions. For each one, a set of inputs, outputs, and a short description of it is provided.

\lstinline[basicstyle=\color{red}]|unpickle|: Loads a file saved using pickle. The input is the path to the file and returns the loaded onject.

\lstinline[basicstyle=\color{red}]|load_data|: The input is the path to the directory containing CIFAR-10 dataset. Then the function loads all the batches, concatenate them and perform z-scoring on all the data based on the mean and variance found in training set. Then based on the input \lstinline[basicstyle=\color{red}]|validation_n| splits a validation set from the training data and return training, validation and test sets. Labels are represented by one-hot coding.

\lstinline[basicstyle=\color{red}]|shuffle|: Performs random shuffling of the input arrays. Note that arrays must have the same size in the second axis.

\lstinline[basicstyle=\color{red}]|flip|: Performs horizontal flipping of the samples in a dataset X.

\lstinline[basicstyle=\color{red}]|softmax|: Softmax activation function.

\lstinline[basicstyle=\color{red}]|sigmoid|: Sigmoid activation function

\lstinline[basicstyle=\color{red}]|forward|: forward path in the network.

\lstinline[basicstyle=\color{red}]|accuracy|: Accepts correct labels and predictions of a model and returns the accuracy of the model in predicting the labels.

\lstinline[basicstyle=\color{red}]|loss|: Cross entropy loss and cost. If we set \lstinline|lamda=0| then the result is the loss and otherwise it is the cost. 

\lstinline[basicstyle=\color{red}]|mbce_loss|: Multiple-MBCE loss.

\lstinline[basicstyle=\color{red}]|compute_gradients|: Calculates the gradient of the cost function with respect to $W$ and $b$.

\lstinline[basicstyle=\color{red}]|fit|: Performs training on the dataset \lstinline|X,Y|. This function can perform flipping agumentation (for bonus part) and training with M-BCE (bonus part). Function returns the final values of $W$ and $b$ with logs. Logs is a dictionart containing accuracy, lost and cost after each epoch for training and validation (if given) data during training.

\lstinline[basicstyle=\color{red}]|evaluate|: Accepts parameters of a model and evaluate it on the input dataset. Then accuracy, cost and loss values are returned using a dictionary.

\lstinline[basicstyle=\color{red}]|gradient_checker|: Used to check if our \lstinline|calculate_gradients| function is working properly. This function initialize model parameter with big values and calculates the gradients of the cost function using both numeric and analytical solutions. Then some statistics are printed which help us evaluate the correctness of out implementations.

\lstinline[basicstyle=\color{red}]|fit_and_plot|: This function is used for the mandatory part of the assignment. It initialize a model, fit it on a given dataset, plot the logs, evaluate on test data and finaly plot the parameters $W$. Plots are saved in a subdirectory of the project.

\lstinline[basicstyle=\color{red}]|mandatory|: Performs tests needed for the first part of the assignment.

\lstinline[basicstyle=\color{red}]|bonus_2_1_1|: First method suggested in the bonus part to enhance the performance of the model. This function trains a network on a bigger training dataset and evaluate it on test data.

\lstinline[basicstyle=\color{red}]|bonus_2_1_2|: This function uses flipping as a agumentation method with probability of 0.5 during training. Then the trained model is evaluated on test data.

\lstinline[basicstyle=\color{red}]|lr_scheduler|: A scheduler for learning rate. Input is the epoch we are currently on and the function returns the learning rate we must use in this epoch. This function is used for learning rate decay during training.

\lstinline[basicstyle=\color{red}]|fit_with_scheduler|: Instead of training a model with a fxed learning rate, uses the scheduler to set the learning rate at each epoch.

\lstinline[basicstyle=\color{red}]|plot_correct_hist|: This function plots the histogram of the probability of the correct classes and the ones correctly classified. This function is used in 2.2.

\lstinline[basicstyle=\color{red}]|bonus_2_2|: Trains two models with cross-entropy with softmax and M-BCE with sigmoid, plots the histograms and loss during training and evaluate them on test data. This function does everything for the second question of the bonus part.


Please note that since these functions were originally implemented in different files, running the uploaded code will probably result in compile or runtime errors. If there you need to run the code, please contact me, and I can provide access to the Github repository containing the project for this assignment.




%%%%%%%% EXTRA TIPS %%%%%%%%
%% If you want to include an figure
%%\begin{figure}[H]
%%\includegraphics[]{Pendulum.jpg}
%%\caption{Sketch of the pendulum}
%%\label{fig:pendulum}
%%\end{figure}

%% for multiple figures in one fig
%\begin{figure}[h]
%	\centering
%	\begin{subfigure}{\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{images/sthfivo.png}
%		\caption{}
%	\end{subfigure}
%	\begin{subfigure}{\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{images/sth.png}
%		\caption{}
%	\end{subfigure}
%	\begin{subfigure}{\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{images/sth.png}
%		\caption{}
%	\end{subfigure}
%	\caption{caption}
%	\label{fig:label}
%\end{figure}


%% You can then reference with \ref{fig:pendulum}


%%\newpage
%\bibliographystyle{apacite}
%\bibliography{ref.bib}

\end{document}